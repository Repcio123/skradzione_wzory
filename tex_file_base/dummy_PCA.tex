\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Sample Article 1}
\author{Remigiusz Drozda}
\date{October 2023}

\begin{document}

\maketitle

\section{Introduction}
\paragraph{Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features. High-dimensional data are very common in biology and arise when multiple features, such as expression of many genes, are measured for each sample. This type of data presents several challenges that PCA mitigates: computational expense and an increased error rate due to multiple test correction when testing each feature for association with an outcome. PCA is an unsupervised learning method and is similar to clustering1—it finds patterns without reference to prior knowledge about whether the samples come from different treatment groups or have phenotypic differences.}

\begin{figure}[h]
\centering
    $z y ^ { 2 } = 4 x ^ { 3 } - g _ { 2 } z ^ { 2 } x - g _ { 3 } z ^ { 3 }$
\end{figure}
\paragraph{PCA reduces data by geometrically projecting them onto lower dimensions called principal components (PCs), with the goal of finding the best summary of the data using a limited number of PCs. The first PC is chosen to minimize the total distance between the data and their projection onto the PC (Fig. 1a). By minimizing this distance, we also maximize the variance of the projected points, σ2 (Fig. 1b). The second (and subsequent) PCs are selected similarly, with the additional requirement that they be uncorrelated with all previous PCs. For example, projection onto PC1 is uncorrelated with projection onto PC2, and we can think of the PCs as geometrically orthogonal. This requirement of no correlation means that the maximum number of PCs possible is either the number of samples or the number of features, whichever is smaller. The PC selection process has the effect of maximizing the correlation (r2) (ref. 2) between data and their projection and is equivalent to carrying out multiple linear regression3,4 on the projected data against each variable of the original data. For example, the projection onto PC2 has maximum r2 when used in multiple regression with PC1.}

\begin{figure}[h]
\centering
    $_ { g f } - i \varepsilon \frac { 1 } { 2 } \int A ^ { 2 } d ^ { 4 } x$
\end{figure}
\end{document}
